# This is an example config yaml file for toy BIDS App "toy_bids_app" version 0.0.6.
# Toy BIDS App's Docker image is publicly available at: https://hub.docker.com/r/pennlinc/toy_bids_app
# This BIDS App counts number of non-hidden files in a subject's (or a session's) folder;
#     More details please see: https://github.com/PennLINC/babs_tests/blob/main/docker/README.md#toy-bids-app-toy_bids_app
# How to prepare a container DataLad dataset of this toy BIDS App? See docs here: https://pennlinc-babs.readthedocs.io/en/latest/preparation_container.html

# Warning!!!
#   This is only an example, which may not necessarily fit your purpose,
#   or be an optimized solution for your case,
#   or be compatible to the toy BIDS App version you're using.
#   Therefore, please change and tailor it for your case before use it!!!

babs_singularity_run:
    --no-zipped: ""  # for raw BIDS dataset
    # --zipped: ""    # for zipped input dataset
    -v: ""

babs_zip_foldername:   # the foldername(s) to get zipped and the version string(s)
    toybidsapp: "0-0-6"     # should be the same as in `babs-init`'s argument `container_name`; better using "-" instead of "."

cluster_resources:
    interpreting_shell: /bin/bash   # "-S /bin/bash" on cubic
    hard_memory_limit: 2G   # "-l h_vmem=25G" on cubic
    temporary_disk_space: 20G   # "-l tmpfree=50G" on cubic  # this is highly-recommended on cubic
    customized_text: |
        #$ -R y

# Users need to add their customized bash command below,
#   they will be used as preambles in `participant_job.sh`
#   the commands should not be quoted!
script_preamble: |
    source ${CONDA_PREFIX}/bin/activate mydatalad    # Penn Med CUBIC cluster; replace 'mydatalad' with your conda env name

# Where to run the jobs:
compute_job_working_dir: "${CBICA_TMPDIR}"   # Penn Med CUBIC cluster tmp space
# "/cbica/comp_space/$(basename $HOME)"   # PennMed CUBIC cluster compute space

# Below is to filter out subjects (or sessions)
#   right now we only filter based on unzipped dataset
# required_files:
#     $INPUT_DATASET_#1:
#         - ""

# Keywords for alerting messages in log files:
keywords_alert:
    # some_other:  # Only used for testing purpose!
    o_file:
        - "xxxx"
        # - "did not provide --session_label"
    e_file:
        - "xxx"
        # - "[INFO] Cloning dataset"
        # - "git remote add outputstore"
