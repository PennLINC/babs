# Below is example config yaml file for toy BIDS App:

singularity_run:
    # --no-zipped: ""  # for raw BIDS dataset
    --zipped: ""    # for zipped input dataset
    -v: ""

zip_foldernames:   # the foldername(s) to get zipped and the version string(s)
    toybidsapp: "0-0-7"     # should be the same as in `babs-init`'s argument `container_name`; better using "-" instead of "."

# environment_variable:
#     SINGULARITYENV_TEMPLATEFLOW_HOME: '~/.cache/templateflow'   # no need, default in babs

cluster_resources:
    interpreting_shell: /bin/bash   # "-S /bin/bash" on cubic
    hard_memory_limit: 25G   # "-l h_vmem=25G" on cubic
    # soft_memory_limit: 23.5G   # "-l s_vmem=23.5G" on cubic   # no need to set s_vmem
    temporary_disk_space: 50G   # "-l tmpfree=200G" on cubic  # this is highly-recommended on cubic
    customized_text: |
        #$ -R y

# Users need to add their customized bash command below,
#   they will be used as preambles in `participant_job.sh`
#   the commands should not be quoted!
script_preamble: |
    source ${CONDA_PREFIX}/bin/activate mydatalad    # Penn Med CUBIC cluster; replace 'mydatalad' with your conda env name

# Where to run the jobs:
job_compute_space: "${CBICA_TMPDIR}"   # Penn Med CUBIC cluster tmp space

# Below is to filter out subjects (or sessions)
#   right now we only filter based on unzipped dataset
# required_files:
#     $INPUT_DATASET_#1:
#         - ""

