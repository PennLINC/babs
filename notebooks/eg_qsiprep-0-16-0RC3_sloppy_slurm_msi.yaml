# This is an example config yaml file for:
#   BIDS App:         QSIPrep ("qsiprep")
#   BIDS App version: 0.16.0RC3
#   Task:             `--sloppy` mode
#   Which system:     Slurm
#   Tested on which cluster:  MSI cluster
# QSIPrep's Docker image is publicly available at: https://hub.docker.com/r/pennbbl/qsiprep

# WARNING!!!
#   This is only an example, which may not necessarily fit your purpose,
#   or be an optimized solution for your case,
#   or be compatible to the BIDS App version you're using.
#   Therefore, please change and tailor it for your case before use it!!!
# WARNING!!!
#   We'll use `--sloppy` testing mode of QSIPrep.
#   Therefore this YAML file should only be used for testing purpose.
#   You should NOT use this YAML file to generate formal results!

singularity_run:
    -v: "-v"
    -w: "$BABS_TMPDIR"   # this is a placeholder. BABS will replace it with `${PWD}/.git/tmp/wkdir`
    --n_cpus: "$SLURM_CPUS_PER_TASK"   # Slurm env variable, taking value from `--cpus-per-task`, i.e., "number_of_cpus" in section "cluster_resources"
    --omp-nthreads: "3"   # without this, only run single-threaded jobs (N=`--n_cpus`) at once
    --stop-on-first-crash: ""
    --fs-license-file: "/home/faird/zhaoc/software/FreeSurfer/license.txt"  # [FIX ME] path to FS license file
    --skip-bids-validation: Null  # Null or NULL is also a placeholder
    --unringing-method: "mrdegibbs"
    --output-resolution: "2.0"
    --sloppy: ""    # WARNING: only use this when testing
    --hmc-model: "none"   # WARNING: only use this when testing
    --dwi-only: ""   # WARNING: only use this when testing

# Output foldername(s) to be zipped, and the BIDS App version to be included in the zip filename(s):
zip_foldernames:
    qsiprep: "0-16-0RC3"     # folder 'qsiprep' will be zipped into 'sub-xx_ses-yy_qsiprep-0-16-0RC3.zip'

cluster_resources:
    interpreting_shell: "/bin/bash -l"   # --> "#!/bin/bash -l"
    hard_memory_limit: 32G   # --> "#SBATCH --mem=32G"
    temporary_disk_space: 200G   # --> "#SBATCH --tmp=200G"
    number_of_cpus: "6"    # --> "#SBATCH --cpus-per-task=6"
    hard_runtime_limit: "48:00:00"    # --> "--time=48:00:00", i.e., 2 days. Should NOT large than partition's time limit!
    customized_text: |
        #SBATCH -p ram256g
# Other choices of job partitions on MSI: amd2tb,ram256g,v100,k40
# Notes: Above `customized_text` is MSI Slurm cluster specific.
#   So it may not be relevant for other clusters

# Users need to add their customized bash command below,
#   they will be used as preambles in `participant_job.sh`
#   the commands should not be quoted!
script_preamble: |
    source /home/faird/shared/code/external/envs/miniconda3/load_miniconda3.sh   # [FIX ME] MSI cluster faird group. Replace filepath with yours.
    conda activate babs   # [FIX ME] replace 'babs' with your env variable name

# Where to run the jobs:
job_compute_space: "/tmp"   # [FIX ME] MSI cluster

# Below is to filter out subjects (or sessions)
#   right now we only filter based on unzipped dataset
required_files:
    $INPUT_DATASET_#1:
        - "dwi/*_dwi.nii*"   # QSIPrep
