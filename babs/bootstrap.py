"""This is the main module."""

import os
import os.path as op
import subprocess
from pathlib import Path

import datalad.api as dlapi
import pandas as pd
import yaml
from jinja2 import Environment, PackageLoader, StrictUndefined

from babs.base import BABS
from babs.container import Container
from babs.input_datasets import InputDatasets
from babs.system import System, validate_queue
from babs.utils import (
    get_datalad_version,
    results_status_columns,
    results_status_default_values,
    status_dtypes,
    validate_processing_level,
)


class BABSBootstrap(BABS):
    """A BABS subclass that implements the bootstrap process."""

    def _apply_config(self):
        pass

    def babs_bootstrap(
        self,
        processing_level,
        queue,
        container_ds,
        container_name,
        container_config,
        initial_inclusion_df=None,
    ):
        """
        Bootstrap a babs project: initialize datalad-tracked RIAs, generate scripts to be used, etc

        Parameters
        ----------
        processing_level: str
            processing level of the BIDS App
        queue: str
            job scheduling system of the cluster
        container_name: str
            name of the container, best to include version number.
            e.g., 'fmriprep-0-0-0'
        container_ds: str
            path to the container datalad dataset which the user provides
        container_config: str
            Path to a YAML file that contains the configurations
            of how to run the BIDS App container
        initial_inclusion_df: pd.DataFrame
            initial inclusion dataframe of subjects/sessions to analyze
        """
        if op.exists(self.project_root):
            raise FileExistsError(
                f'{self.project_root} already exists.\n\n'
                '`babs init` requires path to a non-existent folder.'
            )

        parent_dir = Path(self.project_root).parent
        # check if parent directory exists:
        if not parent_dir.exists():
            raise ValueError(
                f"The parent folder '{parent_dir}' does not exist! `babs init` won't proceed."
            )

        # check if parent directory is writable:
        if not os.access(parent_dir, os.W_OK):
            raise ValueError(
                f"The parent folder '{parent_dir}' is not writable! `babs init` won't proceed."
            )

        os.makedirs(self.project_root)

        # validate `processing_level`:
        self.processing_level = validate_processing_level(processing_level)

        # Read the config yaml to get the datasets:
        with open(container_config) as f:
            babs_config = yaml.safe_load(f)
        # Detect optional pipeline configuration from the provided YAML
        # Store on self so downstream bootstrap logic can branch accordingly
        self.pipeline = babs_config.get('pipeline')
        datasets = babs_config.get('input_datasets')
        if not datasets:
            raise ValueError('No input datasets found in the container config file.')
        self.input_datasets = InputDatasets(processing_level, datasets)
        self.queue = validate_queue(queue)
        system = System(self.queue)

        # Create `analysis` folder: -----------------------------
        print('DataLad version: ' + get_datalad_version())
        print('\nCreating `analysis` folder (also a datalad dataset)...')
        self._analysis_datalad_handle = dlapi.create(
            self.analysis_path, cfg_proc='yoda', annex=True
        )
        self.input_datasets.update_abs_paths(Path(self.analysis_path))
        self.input_datasets.set_inclusion_dataframe(initial_inclusion_df, processing_level)

        # Prepare `.gitignore` ------------------------------
        # write into .gitignore so won't be tracked by git:
        gitignore_path = op.join(self.analysis_path, '.gitignore')
        # if exists already, remove it:
        if op.exists(gitignore_path):
            os.remove(gitignore_path)
        gitignore_file = open(gitignore_path, 'a')  # open in append mode

        # not to track `logs` folder:
        gitignore_file.write('\nlogs')
        # not to track `.*_datalad_lock`:
        gitignore_file.write('\n.*_datalad_lock')
        # not to track lock file:
        gitignore_file.write('\n' + 'code/babs_proj_config.yaml.lock')
        # not to track `job_status.csv`:
        gitignore_file.write('\n' + 'code/job_status.csv')
        gitignore_file.write('\n' + 'code/job_status.csv.lock')
        gitignore_file.write('\n' + 'code/job_submit.csv')
        gitignore_file.write('\n' + 'code/job_submit.csv.lock')
        # not to track files generated by `babs check-setup`:
        gitignore_file.write('\n' + 'code/check_setup/test_job_info.yaml')
        gitignore_file.write('\n' + 'code/check_setup/check_env.yaml')
        gitignore_file.write('\n')

        gitignore_file.close()
        self.datalad_save(path='.gitignore', message='Save .gitignore file')

        # Create `babs_proj_config.yaml` file: ----------------------
        print('Save BABS project configurations in a YAML file ...')
        print("Path to this yaml file will be: 'analysis/code/babs_proj_config.yaml'")

        env = Environment(
            loader=PackageLoader('babs', 'templates'),
            autoescape=False,
            undefined=StrictUndefined,
        )
        template = env.get_template('babs_proj_config.yaml.jinja2')

        with open(self.config_path, 'w') as f:
            f.write(
                template.render(
                    processing_level=self.processing_level,
                    queue=self.queue,
                    input_ds=self.input_datasets,
                    container_name=container_name,
                    container_ds=container_ds,
                )
            )
        self.datalad_save(
            path=self.config_path,
            message='Initial save of babs_proj_config.yaml',
        )
        # Create output RIA sibling: -----------------------------
        print('\nCreating output and input RIA...')
        self.analysis_datalad_handle.create_sibling_ria(
            name='output', url=self.output_ria_url, new_store_ok=True
        )

        self.wtf_key_info()

        # Create input RIA sibling:
        self.analysis_datalad_handle.create_sibling_ria(
            name='input',
            url=self.input_ria_url,
            storage_sibling=False,  # False is `off` in CLI of datalad
            new_store_ok=True,
        )

        # Register the input dataset(s): -----------------------------
        print('\nRegistering the input dataset(s)...')
        for idx, in_ds in enumerate(self.input_datasets):
            # path to cloned dataset:
            dataset_name = in_ds.name
            dataset_source = in_ds.origin_url

            print(f'Cloning input dataset #{idx + 1}: {dataset_name}')

            # clone input dataset(s) as sub-dataset into `analysis` dataset:
            dlapi.clone(
                dataset=self.analysis_path,
                source=dataset_source,
                path=in_ds.babs_project_analysis_path,
            )

            # amend the previous commit with a nicer commit message:
            commit_message = f"Register input data dataset '{dataset_name}' as a subdataset"
            git_cmd = ['git', 'commit', '--amend', '-m', commit_message]

            result = subprocess.run(
                git_cmd,
                cwd=self.analysis_path,
                stdout=subprocess.PIPE,
                check=True,
            )
            result.check_returncode()

        # Perform checks on the inputs:
        self.input_datasets.validate_input_contents()

        # directly add container as sub-dataset of `analysis`:
        print('\nAdding the container as a sub-dataset of `analysis` dataset...')
        dlapi.install(
            dataset=self.analysis_path,
            source=container_ds,  # container datalad dataset
            path=op.join(self.analysis_path, 'containers'),
        )
        # into `analysis/containers` folder

        # Create initial container for sanity check
        container = Container(container_ds, container_name, container_config)

        # sanity check of container ds:
        container.sanity_check(self.analysis_path)

        # ==============================================================
        # Bootstrap scripts:
        # ==============================================================

        # Check if this is a pipeline configuration
        if self.pipeline is not None:
            # Validate all containers in the pipeline
            print(f'\nValidating {len(self.pipeline)} containers in pipeline...')
            containers = []
            for i, step in enumerate(self.pipeline):
                step_container_name = step['container_name']
                print(f'Validating container {i + 1}/{len(self.pipeline)}: {step_container_name}')
                step_container = Container(container_ds, step_container_name, container_config)
                step_container.sanity_check(self.analysis_path)
                containers.append(step_container)

            self._bootstrap_pipeline_scripts(container_ds, container_config, system)
            # Use first container for compatibility with existing code
            container = containers[0]
        else:
            self._bootstrap_single_app_scripts(
                container_ds, container_name, container_config, system
            )
            container = Container(container_ds, container_name, container_config)

        # Copy in any other files needed:
        self._init_import_files(container.config.get('imported_files', []))
        # Create the inclusion file
        self._update_inclusion_dataframe(initial_inclusion_df)

        # Generate the template of job submission: --------------------------------
        print('\nGenerating templates for job submission calls...')
        if self.pipeline is not None:
            # Generate templates for all pipeline containers
            for i, step in enumerate(self.pipeline):
                step_container_name = step['container_name']
                print(
                    f'Generating job template for container {i + 1}/{len(self.pipeline)}: '
                    f'{step_container_name}'
                )
                step_container = Container(container_ds, step_container_name, container_config)

                # Main job template (use first container for main template)
                if i == 0:
                    yaml_path = op.join(self.analysis_path, 'code', 'submit_job_template.yaml')
                    step_container.generate_job_submit_template(yaml_path, self, system)

                # Test job template for each container
                yaml_test_path = op.join(
                    self.analysis_path,
                    'code/check_setup',
                    f'submit_test_job_template_step_{i + 1}_{step_container_name}.yaml',
                )
                step_container.generate_job_submit_template(
                    yaml_test_path, self, system, test=True
                )
        else:
            # Single container case
            print('The template text file will be named as `submit_job_template.yaml`.')
            yaml_path = op.join(self.analysis_path, 'code', 'submit_job_template.yaml')
            container.generate_job_submit_template(yaml_path, self, system)

            # also, generate template for testing job used by `babs check-setup`:
            yaml_test_path = op.join(
                self.analysis_path, 'code/check_setup', 'submit_test_job_template.yaml'
            )
            container.generate_job_submit_template(yaml_test_path, self, system, test=True)

        # datalad save:
        if self.pipeline is not None:
            # Save all pipeline template files
            template_paths = ['code/submit_job_template.yaml']
            for i, step in enumerate(self.pipeline):
                step_container_name = step['container_name']
                template_paths.append(
                    f'code/check_setup/submit_test_job_template_step_{i + 1}_'
                    f'{step_container_name}.yaml'
                )
            self.datalad_save(
                path=template_paths,
                message='Templates for pipeline job submission',
            )
        else:
            # Single container case
            self.datalad_save(
                path=[
                    'code/submit_job_template.yaml',
                    'code/check_setup/submit_test_job_template.yaml',
                ],
                message='Template for job submission',
            )

        # Finish up and get ready for clusters running: -----------------------
        # create folder `logs` in `analysis`; future log files go here
        #   this won't be tracked by git (as already added to `.gitignore`)
        log_path = op.join(self.analysis_path, 'logs')
        if not op.exists(log_path):
            os.makedirs(log_path)

        # in case anything in `code/` was not saved:
        #   If there is anything not saved yet, probably should be added to `.gitignore`
        #   at the beginning of `babs init`.
        self.datalad_save(
            path='code/', message="Save anything in folder code/ that hasn't been saved"
        )

        print('\nFinal steps...')
        # No need to keep the input dataset(s):
        #   old version: datalad uninstall -r --nocheck inputs/data
        print("DataLad dropping input dataset's contents...")
        for in_ds in self.input_datasets:
            _ = self.analysis_datalad_handle.drop(
                path=in_ds.babs_project_analysis_path,
                recursive=True,  # and potential subdataset
                reckless='availability',
            )
            # not to check availability
            # seems have to specify the dataset (by above `handle`);
            # otherwise, dl thinks the dataset is where current python is running

        # Update input and output RIA:
        print('Updating input and output RIA...')
        #   datalad push --to input
        #   datalad push --to output
        self.analysis_datalad_handle.push(to='input')
        self.analysis_datalad_handle.push(to='output')

        # Add an alias to the data in output RIA store:
        print("Adding an alias 'data' to output RIA store...")
        """
        RIA_DIR=$(find $PROJECTROOT/output_ria/???/ -maxdepth 1 -type d | sort | tail -n 1)
        mkdir -p ${PROJECTROOT}/output_ria/alias
        ln -s ${RIA_DIR} ${PROJECTROOT}/output_ria/alias/data
        """
        if not op.exists(op.join(self.output_ria_path, 'alias')):
            os.makedirs(op.join(self.output_ria_path, 'alias'))
        # create a symbolic link:
        the_symlink = op.join(self.output_ria_path, 'alias', 'data')
        if op.exists(the_symlink) & op.islink(the_symlink):
            # exists and is a symlink: remove first
            os.remove(the_symlink)
        os.symlink(self.output_ria_data_dir, the_symlink)
        # to check this symbolic link, just: $ ls -l <output_ria/alias/data>
        #   it should point to /full/path/output_ria/xxx/xxx-xxx-xxx-xxx

        # Initialize the job status csv file:
        self._create_initial_job_status_csv()

        print('\n')
        print(
            'BABS project has been initialized!'
            " Path to this BABS project: '" + self.project_root + "'"
        )
        print('`babs init` was successful!')

    def _bootstrap_single_app_scripts(
        self, container_ds, container_name, container_config, system
    ):
        """Bootstrap scripts for single BIDS app configuration."""
        container = Container(container_ds, container_name, container_config)

        # Generate `<containerName>_zip.sh`: ----------------------------------
        # which is a bash script of singularity run + zip
        # in folder: `analysis/code`
        print('\nGenerating a bash script for running container and zipping the outputs...')
        print('This bash script will be named as `' + container_name + '_zip.sh`')
        bash_path = op.join(self.analysis_path, 'code', container_name + '_zip.sh')
        container.generate_bash_run_bidsapp(bash_path, self.input_datasets, self.processing_level)
        self.datalad_save(
            path='code/' + container_name + '_zip.sh',
            message='Generate script of running container',
        )

        # make another folder within `code` for test jobs:
        os.makedirs(op.join(self.analysis_path, 'code/check_setup'), exist_ok=True)

        # Generate `participant_job.sh`: --------------------------------------
        print('\nGenerating a bash script for running jobs at participant (or session) level...')
        print('This bash script will be named as `participant_job.sh`')
        bash_path = op.join(self.analysis_path, 'code', 'participant_job.sh')
        container.generate_bash_participant_job(
            bash_path, self.input_datasets, self.processing_level, system
        )

        # also, generate a bash script of a test job used by `babs check-setup`:
        path_check_setup = op.join(self.analysis_path, 'code/check_setup')
        container.generate_bash_test_job(path_check_setup, system)

    def _bootstrap_pipeline_scripts(self, container_ds, container_config, system):
        """Bootstrap scripts for pipeline configuration."""
        from babs.generate_bidsapp_runscript import generate_pipeline_runscript
        from babs.generate_submit_script import generate_submit_script

        print('\nGenerating pipeline scripts...')

        # Prepare container images for submit script
        container_images = [
            f'containers/.datalad/environments/{s["container_name"]}/image' for s in self.pipeline
        ]

        # Compute final zip foldernames from the last step's config
        from babs.utils import app_output_settings_from_config

        last_step_config = self.pipeline[-1].get('config', {})
        final_zip_foldernames, _ = app_output_settings_from_config(last_step_config)

        # Generate pipeline run script using unified pipeline generator
        pipeline_script_path = op.join(self.analysis_path, 'code', 'pipeline_zip.sh')
        templateflow_home = os.getenv('TEMPLATEFLOW_HOME')

        pipeline_script_content = generate_pipeline_runscript(
            pipeline_config=self.pipeline,
            processing_level=self.processing_level,
            input_datasets=self.input_datasets.as_records(),
            templateflow_home=templateflow_home,
        )

        with open(pipeline_script_path, 'w') as f:
            f.write(pipeline_script_content)
        os.chmod(pipeline_script_path, 0o700)

        self.datalad_save(
            path='code/pipeline_zip.sh',
            message='Generate pipeline run script',
        )

        # make another folder within `code` for test jobs:
        os.makedirs(op.join(self.analysis_path, 'code/check_setup'), exist_ok=True)

        # Generate `participant_job.sh`: --------------------------------------
        print('\nGenerating a bash script for running jobs at participant (or session) level...')
        print('This bash script will be named as `participant_job.sh`')
        bash_path = op.join(self.analysis_path, 'code', 'participant_job.sh')

        # Get system config for submit script
        system_config = system.get_cluster_resources_config()

        # Aggregate resource requirements from all pipeline steps
        aggregated_resources = self._aggregate_pipeline_resources(system_config)

        participant_job_content = generate_submit_script(
            queue_system=self.queue,
            cluster_resources_config=aggregated_resources,
            script_preamble=system_config.get('script_preamble', ''),
            job_scratch_directory=system_config.get('job_compute_space', '/tmp'),
            input_datasets=self.input_datasets.as_records(),
            processing_level=self.processing_level,
            container_name='pipeline',  # placeholder
            zip_foldernames=final_zip_foldernames,
            run_script_relpath='code/pipeline_zip.sh',
            container_images=container_images,
            datalad_run_message='pipeline',
        )

        with open(bash_path, 'w') as f:
            f.write(participant_job_content)
        os.chmod(bash_path, 0o700)

        # also, generate bash scripts of test jobs used by `babs check-setup`:
        # Generate test jobs for all containers in the pipeline
        path_check_setup = op.join(self.analysis_path, 'code/check_setup')
        for i, step in enumerate(self.pipeline):
            step_container_name = step['container_name']
            print(
                f'Generating test job for container {i + 1}/{len(self.pipeline)}: '
                f'{step_container_name}'
            )
            step_container = Container(container_ds, step_container_name, container_config)
            # Create separate test job directories for each container
            step_check_setup = op.join(path_check_setup, f'step_{i + 1}_{step_container_name}')
            os.makedirs(step_check_setup, exist_ok=True)
            step_container.generate_bash_test_job(step_check_setup, system)

    def _aggregate_pipeline_resources(self, base_system_config):
        """
        Aggregate resource requirements from all pipeline steps.

        Parameters
        ----------
        base_system_config : dict
            Base system configuration

        Returns
        -------
        dict
            Aggregated resource configuration
        """
        aggregated = base_system_config.copy()

        # Track maximum resource requirements across all steps
        max_cpus = 0
        max_memory = 0
        max_runtime = 0

        print(f'\nAggregating resource requirements from {len(self.pipeline)} pipeline steps...')

        for i, step in enumerate(self.pipeline):
            step_config = step.get('config', {})
            cluster_resources = step_config.get('cluster_resources', {})
            step_name = step['container_name']

            print(f'  Step {i + 1} ({step_name}):')

            # Extract numeric values for comparison
            step_cpus = cluster_resources.get('cpus', 0)
            step_memory = cluster_resources.get('memory', 0)
            step_runtime = cluster_resources.get('runtime', 0)

            if step_cpus:
                max_cpus = max(max_cpus, int(step_cpus))
                print(f'    CPUs: {step_cpus}')
            if step_memory:
                max_memory = max(max_memory, int(step_memory))
                print(f'    Memory: {step_memory}')
            if step_runtime:
                max_runtime = max(max_runtime, int(step_runtime))
                print(f'    Runtime: {step_runtime}')

        # Update aggregated config with maximum values
        if max_cpus > 0:
            aggregated['cpus'] = max_cpus
            print('\nFinal aggregated resources:')
            print(f'  CPUs: {max_cpus}')
        if max_memory > 0:
            aggregated['memory'] = max_memory
            print(f'  Memory: {max_memory}')
        if max_runtime > 0:
            aggregated['runtime'] = max_runtime
            print(f'  Runtime: {max_runtime}')

        return aggregated

    def _init_import_files(self, file_list):
        """
        Import files into the BABS project and datalad save.

        Parameters
        ----------
        file_list: list
            List of dictionaries containing the following keys:
            - 'original_path': str
                The path to the file in the BABS project
            - 'analysis_path': str
                The path to the file in the analysis folder
        """
        imported_files = []
        for imported_file in file_list:
            # Check that the file exists:
            if not op.exists(imported_file['original_path']):
                raise FileNotFoundError(
                    f'Requested imported file {imported_file["original_path"]} does not exist.'
                )
            imported_location = op.join(self.analysis_path, imported_file['analysis_path'])
            # Copy the file using pure Python:
            with (
                open(imported_file['original_path'], 'rb') as src,
                open(imported_location, 'wb') as dst,
            ):
                dst.write(src.read())
            if not op.exists(imported_location):
                raise FileNotFoundError(
                    f'Failed to copy file {imported_file["original_path"]} to {imported_location}'
                )
            # Append the relative path instead of absolute path
            imported_files.append(op.relpath(imported_location, self.analysis_path))
        if imported_files:
            self.datalad_save(
                path=imported_files,
                message='Import files',
            )

    def clean_up(self):
        """
        If `babs init` failed, this function cleans up the BABS project `babs init` creates.

        Notes
        -----
        Steps in `babs init`:
        * create `analysis` datalad dataset
        * create input and output RIA
        * clone input dataset(s)
        * generate bootstrapped scripts
        * finish up
        """
        if op.exists(self.project_root):  # if BABS project root folder has been created:
            if op.exists(self.analysis_path):  # analysis folder is created by datalad
                print('Removing input dataset(s) if cloned...')
                for in_ds in self.input_datasets:
                    if op.exists(in_ds.babs_project_analysis_path):
                        # use `datalad remove` to remove:
                        _ = self.analysis_datalad_handle.remove(
                            path=in_ds.babs_project_analysis_path, reckless='modification'
                        )

                # `git annex dead here`:
                print('\nRunning `git annex dead here`...')
                proc_git_annex_dead = subprocess.run(
                    ['git', 'annex', 'dead', 'here'],
                    cwd=self.analysis_path,
                    stdout=subprocess.PIPE,
                )
                proc_git_annex_dead.check_returncode()

                # Update input and output RIA:
                print('\nUpdating input and output RIA if created...')
                #   datalad push --to input
                #   datalad push --to output
                if op.exists(self.input_ria_path):
                    self.analysis_datalad_handle.push(to='input')
                if op.exists(self.output_ria_path):
                    self.analysis_datalad_handle.push(to='output')

            # Now we can delete this project folder:
            print('\nDeleting created BABS project folder...')
            proc_rm_project_folder = subprocess.run(
                ['rm', '-rf', self.project_root], stdout=subprocess.PIPE
            )
            proc_rm_project_folder.check_returncode()

        # confirm the BABS project has been removed:
        assert not op.exists(self.project_root), (
            'Created BABS project was not completely deleted!'
            " Path to created BABS project: '" + self.project_root + "'"
        )

        print('\nCreated BABS project has been cleaned up.')

    def _create_initial_job_status_csv(self):
        """
        Create the initial job status csv file.
        """
        if op.exists(self.job_status_path_abs):
            return

        # Load the complete list of subjects and optionally sessions
        df_sub = pd.read_csv(self.list_sub_path_abs)
        df_job = df_sub.copy()

        # Fill the columns that should get default values
        for column_name, default_value in results_status_default_values.items():
            df_job[column_name] = default_value

        # ensure dtypes for all the columns
        for column_name in results_status_columns:
            df_job[column_name] = df_job[column_name].astype(status_dtypes[column_name])

        df_job.to_csv(self.job_status_path_abs, index=False)
