"""This is the main module."""

import os
import os.path as op
import subprocess
from pathlib import Path

import datalad.api as dlapi
import pandas as pd
import yaml
from jinja2 import Environment, PackageLoader, StrictUndefined

from babs.base import BABS
from babs.container import Container
from babs.input_datasets import InputDatasets
from babs.system import System, validate_queue
from babs.utils import (
    combine_inclusion_dataframes,
    get_datalad_version,
    results_status_columns,
    results_status_default_values,
    status_dtypes,
    validate_processing_level,
)


class BABSBootstrap(BABS):
    """A BABS subclass that implements the bootstrap process."""

    def _apply_config(self):
        pass

    def babs_bootstrap(
        self,
        processing_level,
        queue,
        container_ds,
        container_name,
        container_config,
        initial_inclusion_df=None,
    ):
        """
        Bootstrap a babs project: initialize datalad-tracked RIAs, generate scripts to be used, etc

        Parameters
        ----------
        processing_level: str
            processing level of the BIDS App
        queue: str
            job scheduling system of the cluster
        container_name: str
            name of the container, best to include version number.
            e.g., 'fmriprep-0-0-0'
        container_ds: str
            path to the container datalad dataset which the user provides
        container_config: str
            Path to a YAML file that contains the configurations
            of how to run the BIDS App container
        initial_inclusion_df: pd.DataFrame
            initial inclusion dataframe of subjects/sessions to analyze
        """
        if op.exists(self.project_root):
            raise FileExistsError(
                f'{self.project_root} already exists.\n\n'
                '`babs init` requires path to a non-existent folder.'
            )

        parent_dir = Path(self.project_root).parent
        # check if parent directory exists:
        if not parent_dir.exists():
            raise ValueError(
                f"The parent folder '{parent_dir}' does not exist! `babs init` won't proceed."
            )

        # check if parent directory is writable:
        if not os.access(parent_dir, os.W_OK):
            raise ValueError(
                f"The parent folder '{parent_dir}' is not writable! `babs init` won't proceed."
            )

        os.makedirs(self.project_root)

        # validate `processing_level`:
        self.processing_level = validate_processing_level(processing_level)

        # Read the config yaml to get the datasets:
        with open(container_config) as f:
            babs_config = yaml.safe_load(f)
        datasets = babs_config.get('input_datasets')
        if not datasets:
            raise ValueError('No input datasets found in the container config file.')
        self.input_datasets = InputDatasets(processing_level, datasets)
        self.queue = validate_queue(queue)
        system = System(self.queue)

        # Create `analysis` folder: -----------------------------
        print('DataLad version: ' + get_datalad_version())
        print('\nCreating `analysis` folder (also a datalad dataset)...')
        self.analysis_datalad_handle = dlapi.create(
            self.analysis_path, cfg_proc='yoda', annex=True
        )
        self.input_datasets.update_abs_paths(Path(self.analysis_path))
        self.input_datasets.set_inclusion_dataframe(initial_inclusion_df, processing_level)

        # Prepare `.gitignore` ------------------------------
        # write into .gitignore so won't be tracked by git:
        gitignore_path = op.join(self.analysis_path, '.gitignore')
        # if exists already, remove it:
        if op.exists(gitignore_path):
            os.remove(gitignore_path)
        gitignore_file = open(gitignore_path, 'a')  # open in append mode

        # not to track `logs` folder:
        gitignore_file.write('\nlogs')
        # not to track `.*_datalad_lock`:
        gitignore_file.write('\n.*_datalad_lock')
        # not to track lock file:
        gitignore_file.write('\n' + 'code/babs_proj_config.yaml.lock')
        # not to track `job_status.csv`:
        gitignore_file.write('\n' + 'code/job_status.csv')
        gitignore_file.write('\n' + 'code/job_status.csv.lock')
        # not to track files generated by `babs check-setup`:
        gitignore_file.write('\n' + 'code/check_setup/test_job_info.yaml')
        gitignore_file.write('\n' + 'code/check_setup/check_env.yaml')
        gitignore_file.write('\n')

        gitignore_file.close()
        self.datalad_save(path='.gitignore', message='Save .gitignore file')

        # Create `babs_proj_config.yaml` file: ----------------------
        print('Save BABS project configurations in a YAML file ...')
        print("Path to this yaml file will be: 'analysis/code/babs_proj_config.yaml'")

        env = Environment(
            loader=PackageLoader('babs', 'templates'),
            autoescape=False,
            undefined=StrictUndefined,
        )
        template = env.get_template('babs_proj_config.yaml.jinja2')

        with open(self.config_path, 'w') as f:
            f.write(
                template.render(
                    processing_level=self.processing_level,
                    queue=self.queue,
                    input_ds=self.input_datasets,
                    container_name=container_name,
                    container_ds=container_ds,
                )
            )
        self.datalad_save(
            path=self.config_path,
            message='Initial save of babs_proj_config.yaml',
        )
        # Create output RIA sibling: -----------------------------
        print('\nCreating output and input RIA...')
        self.analysis_datalad_handle.create_sibling_ria(
            name='output', url=self.output_ria_url, new_store_ok=True
        )

        self.wtf_key_info()

        # Create input RIA sibling:
        self.analysis_datalad_handle.create_sibling_ria(
            name='input',
            url=self.input_ria_url,
            storage_sibling=False,  # False is `off` in CLI of datalad
            new_store_ok=True,
        )

        # Register the input dataset(s): -----------------------------
        print('\nRegistering the input dataset(s)...')
        for idx, in_ds in enumerate(self.input_datasets):
            # path to cloned dataset:
            dataset_name = in_ds.name
            dataset_source = in_ds.origin_url

            print(f'Cloning input dataset #{idx + 1}: {dataset_name}')

            # clone input dataset(s) as sub-dataset into `analysis` dataset:
            dlapi.clone(
                dataset=self.analysis_path,
                source=dataset_source,
                path=in_ds.babs_project_analysis_path,
            )

            # amend the previous commit with a nicer commit message:
            commit_message = f"Register input data dataset '{dataset_name}' as a subdataset"
            git_cmd = ['git', 'commit', '--amend', '-m', commit_message]

            result = subprocess.run(
                git_cmd,
                cwd=self.analysis_path,
                stdout=subprocess.PIPE,
                check=True,
            )
            result.check_returncode()

        # Perform checks on the inputs:
        self.input_datasets.validate_input_contents()

        # directly add container as sub-dataset of `analysis`:
        print('\nAdding the container as a sub-dataset of `analysis` dataset...')
        dlapi.install(
            dataset=self.analysis_path,
            source=container_ds,  # container datalad dataset
            path=op.join(self.analysis_path, 'containers'),
        )
        # into `analysis/containers` folder

        container = Container(container_ds, container_name, container_config)

        # sanity check of container ds:
        container.sanity_check(self.analysis_path)

        # ==============================================================
        # Bootstrap scripts:
        # ==============================================================

        # Generate `<containerName>_zip.sh`: ----------------------------------
        # which is a bash script of singularity run + zip
        # in folder: `analysis/code`
        print('\nGenerating a bash script for running container and zipping the outputs...')
        print('This bash script will be named as `' + container_name + '_zip.sh`')
        bash_path = op.join(self.analysis_path, 'code', container_name + '_zip.sh')
        container.generate_bash_run_bidsapp(bash_path, self.input_datasets, self.processing_level)
        self.datalad_save(
            path='code/' + container_name + '_zip.sh',
            message='Generate script of running container',
        )

        # make another folder within `code` for test jobs:
        os.makedirs(op.join(self.analysis_path, 'code/check_setup'), exist_ok=True)

        # Generate `participant_job.sh`: --------------------------------------
        print('\nGenerating a bash script for running jobs at participant (or session) level...')
        print('This bash script will be named as `participant_job.sh`')
        bash_path = op.join(self.analysis_path, 'code', 'participant_job.sh')
        container.generate_bash_participant_job(
            bash_path, self.input_datasets, self.processing_level, system
        )

        # also, generate a bash script of a test job used by `babs check-setup`:
        path_check_setup = op.join(self.analysis_path, 'code/check_setup')
        container.generate_bash_test_job(path_check_setup, system)

        self.datalad_save(
            path=[
                'code/participant_job.sh',
                'code/check_setup/call_test_job.sh',
                'code/check_setup/test_job.py',
            ],
            message='Participant compute job implementation',
        )
        # NOTE: `dlapi.save()` does not work...
        # e.g., datalad save -m "Participant compute job implementation"

        # Copy in any other files needed:
        self._init_import_files(container.config.get('imported_files', []))

        print('\nDetermining the list of subjects (and sessions) to analyze...')
        sub_ses_inclusion_df = self.input_datasets.generate_inclusion_dataframe()

        # If the user sent an initial inclusion dataframe, combine it with the one
        # generated by the input datasets
        if initial_inclusion_df is not None:
            sub_ses_inclusion_df = combine_inclusion_dataframes(
                [initial_inclusion_df, sub_ses_inclusion_df]
            )
            if sub_ses_inclusion_df.empty:
                raise ValueError(
                    'No subjects/sessions to analyze!'
                    ' Please check the inclusion file you provided.'
                )
            if sub_ses_inclusion_df.shape[0] < initial_inclusion_df.shape[0]:
                print(
                    'Warning: The initial inclusion dataframe you provided '
                    'contains fewer subjects/sessions than the input datasets.'
                )
        # Create the inclusion file
        sub_ses_inclusion_df.to_csv(self.list_sub_path_abs, index=False)
        self.datalad_save(
            path=self.list_sub_path_abs,
            message='Record of inclusion/exclusion of participants/sessions',
        )

        # Generate the template of job submission: --------------------------------
        print('\nGenerating a template for job submission calls...')
        print('The template text file will be named as `submit_job_template.yaml`.')
        yaml_path = op.join(self.analysis_path, 'code', 'submit_job_template.yaml')
        container.generate_job_submit_template(yaml_path, self, system)

        # also, generate template for testing job used by `babs check-setup`:
        yaml_test_path = op.join(
            self.analysis_path, 'code/check_setup', 'submit_test_job_template.yaml'
        )
        container.generate_job_submit_template(yaml_test_path, self, system, test=True)

        # datalad save:
        self.datalad_save(
            path=[
                'code/submit_job_template.yaml',
                'code/check_setup/submit_test_job_template.yaml',
            ],
            message='Template for job submission',
        )

        # Finish up and get ready for clusters running: -----------------------
        # create folder `logs` in `analysis`; future log files go here
        #   this won't be tracked by git (as already added to `.gitignore`)
        log_path = op.join(self.analysis_path, 'logs')
        if not op.exists(log_path):
            os.makedirs(log_path)

        # in case anything in `code/` was not saved:
        #   If there is anything not saved yet, probably should be added to `.gitignore`
        #   at the beginning of `babs init`.
        self.datalad_save(
            path='code/', message="Save anything in folder code/ that hasn't been saved"
        )

        print('\nFinal steps...')
        # No need to keep the input dataset(s):
        #   old version: datalad uninstall -r --nocheck inputs/data
        print("DataLad dropping input dataset's contents...")
        for in_ds in self.input_datasets:
            _ = self.analysis_datalad_handle.drop(
                path=in_ds.babs_project_analysis_path,
                recursive=True,  # and potential subdataset
                reckless='availability',
            )
            # not to check availability
            # seems have to specify the dataset (by above `handle`);
            # otherwise, dl thinks the dataset is where current python is running

        # Update input and output RIA:
        print('Updating input and output RIA...')
        #   datalad push --to input
        #   datalad push --to output
        self.analysis_datalad_handle.push(to='input')
        self.analysis_datalad_handle.push(to='output')

        # Add an alias to the data in output RIA store:
        print("Adding an alias 'data' to output RIA store...")
        """
        RIA_DIR=$(find $PROJECTROOT/output_ria/???/ -maxdepth 1 -type d | sort | tail -n 1)
        mkdir -p ${PROJECTROOT}/output_ria/alias
        ln -s ${RIA_DIR} ${PROJECTROOT}/output_ria/alias/data
        """
        if not op.exists(op.join(self.output_ria_path, 'alias')):
            os.makedirs(op.join(self.output_ria_path, 'alias'))
        # create a symbolic link:
        the_symlink = op.join(self.output_ria_path, 'alias', 'data')
        if op.exists(the_symlink) & op.islink(the_symlink):
            # exists and is a symlink: remove first
            os.remove(the_symlink)
        os.symlink(self.output_ria_data_dir, the_symlink)
        # to check this symbolic link, just: $ ls -l <output_ria/alias/data>
        #   it should point to /full/path/output_ria/xxx/xxx-xxx-xxx-xxx

        # Initialize the job status csv file:
        self._create_initial_job_status_csv()

        print('\n')
        print(
            'BABS project has been initialized!'
            " Path to this BABS project: '" + self.project_root + "'"
        )
        print('`babs init` was successful!')

    def _init_import_files(self, file_list):
        """
        Import files into the BABS project and datalad save.

        Parameters
        ----------
        file_list: list
            List of dictionaries containing the following keys:
            - 'original_path': str
                The path to the file in the BABS project
            - 'analysis_path': str
                The path to the file in the analysis folder
        """
        imported_files = []
        for imported_file in file_list:
            # Check that the file exists:
            if not op.exists(imported_file['original_path']):
                raise FileNotFoundError(
                    f'Requested imported file {imported_file["original_path"]} does not exist.'
                )
            imported_location = op.join(self.analysis_path, imported_file['analysis_path'])
            # Copy the file using pure Python:
            with (
                open(imported_file['original_path'], 'rb') as src,
                open(imported_location, 'wb') as dst,
            ):
                dst.write(src.read())
            if not op.exists(imported_location):
                raise FileNotFoundError(
                    f'Failed to copy file {imported_file["original_path"]} to {imported_location}'
                )
            # Append the relative path instead of absolute path
            imported_files.append(op.relpath(imported_location, self.analysis_path))
        if imported_files:
            self.datalad_save(
                path=imported_files,
                message='Import files',
            )

    def clean_up(self):
        """
        If `babs init` failed, this function cleans up the BABS project `babs init` creates.

        Notes
        -----
        Steps in `babs init`:
        * create `analysis` datalad dataset
        * create input and output RIA
        * clone input dataset(s)
        * generate bootstrapped scripts
        * finish up
        """
        if op.exists(self.project_root):  # if BABS project root folder has been created:
            if op.exists(self.analysis_path):  # analysis folder is created by datalad
                self.analysis_datalad_handle = dlapi.Dataset(self.analysis_path)

                print('Removing input dataset(s) if cloned...')
                for in_ds in self.input_datasets:
                    if op.exists(in_ds.babs_project_analysis_path):
                        # use `datalad remove` to remove:
                        _ = self.analysis_datalad_handle.remove(
                            path=in_ds.babs_project_analysis_path, reckless='modification'
                        )

                # `git annex dead here`:
                print('\nRunning `git annex dead here`...')
                proc_git_annex_dead = subprocess.run(
                    ['git', 'annex', 'dead', 'here'],
                    cwd=self.analysis_path,
                    stdout=subprocess.PIPE,
                )
                proc_git_annex_dead.check_returncode()

                # Update input and output RIA:
                print('\nUpdating input and output RIA if created...')
                #   datalad push --to input
                #   datalad push --to output
                if op.exists(self.input_ria_path):
                    self.analysis_datalad_handle.push(to='input')
                if op.exists(self.output_ria_path):
                    self.analysis_datalad_handle.push(to='output')

            # Now we can delete this project folder:
            print('\nDeleting created BABS project folder...')
            proc_rm_project_folder = subprocess.run(
                ['rm', '-rf', self.project_root], stdout=subprocess.PIPE
            )
            proc_rm_project_folder.check_returncode()

        # confirm the BABS project has been removed:
        assert not op.exists(self.project_root), (
            'Created BABS project was not completely deleted!'
            " Path to created BABS project: '" + self.project_root + "'"
        )

        print('\nCreated BABS project has been cleaned up.')

    def _create_initial_job_status_csv(self):
        """
        Create the initial job status csv file.
        """
        if op.exists(self.job_status_path_abs):
            return

        # Load the complete list of subjects and optionally sessions
        df_sub = pd.read_csv(self.list_sub_path_abs)
        df_job = df_sub.copy()

        # Fill the columns that should get default values
        for column_name, default_value in results_status_default_values.items():
            df_job[column_name] = default_value

        # ensure dtypes for all the columns
        for column_name in results_status_columns:
            df_job[column_name] = df_job[column_name].astype(status_dtypes[column_name])

        df_job.to_csv(self.job_status_path_abs, index=False)

    def datalad_save(self, path, message=None, filter_files=None):
        """
        Save the current status of datalad dataset `analysis`
        Also checks that all the statuses returned are "ok" (or "notneeded")

        Parameters
        ----------
        path: str or list of str
            the path to the file(s) or folder(s) to save
        message: str or None
            commit message in `datalad save`
        filter_files: list of str or None
            list of filenames to exclude from saving
            if None, no files will be filtered

        Notes
        -----
        If the path does not exist, the status will be "notneeded", and won't be error message
            And there won't be a commit with that message
        """
        if filter_files is not None:
            # Create a temporary .gitignore file to exclude specified files
            gitignore_path = os.path.join(self.analysis_path, '.gitignore')
            with open(gitignore_path, 'w') as f:
                for file in filter_files:
                    f.write(f'{file}\n')

            try:
                statuses = self.analysis_datalad_handle.save(path=path, message=message)
            finally:
                # Clean up the temporary .gitignore file
                if os.path.exists(gitignore_path):
                    os.remove(gitignore_path)
        else:
            statuses = self.analysis_datalad_handle.save(path=path, message=message)

        # ^^ number of dicts in list `statuses` = len(path)
        # check that all statuses returned are "okay":
        # below is from cubids
        saved_status = {status['status'] for status in statuses}
        if saved_status.issubset({'ok', 'notneeded'}) is False:
            # exists element in `saved_status` that is not "ok" or "notneeded"
            # ^^ "notneeded": nothing to save
            raise Exception('`datalad save` failed!')
